{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfda7a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading necessary Python libraries\n",
    "from src.llm_eval import llm_eval\n",
    "from src.utils import (\n",
    "    display_results,\n",
    "    list_subjects,\n",
    "    load_and_prepare_dataset,\n",
    "    load_config,\n",
    "    load_json_safely,\n",
    "    load_model_and_tokenizer,\n",
    "    print_random_examples,\n",
    "    random_subject,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "380dda15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: Qwen/Qwen3-1.7B\n",
      "Max New Tokens: 5000\n",
      "Quantization: 4bit\n",
      "Batch status: False\n",
      "Batch size: 1\n",
      "Dataset Name: cais/mmlu\n",
      "Number Examples: 1000\n",
      "Number Subjects: 999\n",
      "Results Path: ./results/result_\n",
      "Log Path: ./logs/log_\n"
     ]
    }
   ],
   "source": [
    "# Load configuration file\n",
    "config_dict = load_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49a7667f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model 'Qwen/Qwen3-1.7B' with quantization: 4bit and batching: False\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91ad75c1474a4ae09cdbe62b08294cf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "model_dict = load_model_and_tokenizer(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53ded2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset 'cais/mmlu'... \n",
      "\n",
      "Number of questions loaded: 1000\n"
     ]
    }
   ],
   "source": [
    "# Load the MMLU dataset\n",
    "dataset = load_and_prepare_dataset(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00689c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique subjects in the dataset: \n",
      "\n",
      "• nutrition \n",
      "\n",
      "• human_aging \n",
      "\n",
      "• prehistory \n",
      "\n",
      "• jurisprudence \n",
      "\n",
      "• computer_security \n",
      "\n",
      "• medical_genetics \n",
      "\n",
      "• high_school_statistics \n",
      "\n",
      "• public_relations \n",
      "\n",
      "• college_physics \n",
      "\n",
      "• global_facts \n",
      "\n",
      "• formal_logic \n",
      "\n",
      "• college_computer_science \n",
      "\n",
      "• clinical_knowledge \n",
      "\n",
      "• high_school_biology \n",
      "\n",
      "• international_law \n",
      "\n",
      "• marketing \n",
      "\n",
      "• us_foreign_policy \n",
      "\n",
      "• high_school_physics \n",
      "\n",
      "• college_mathematics \n",
      "\n",
      "• management \n",
      "\n",
      "• astronomy \n",
      "\n",
      "• college_biology \n",
      "\n",
      "• professional_medicine \n",
      "\n",
      "• elementary_mathematics \n",
      "\n",
      "• high_school_microeconomics \n",
      "\n",
      "• human_sexuality \n",
      "\n",
      "• college_medicine \n",
      "\n",
      "• virology \n",
      "\n",
      "• econometrics \n",
      "\n",
      "• professional_accounting \n",
      "\n",
      "• philosophy \n",
      "\n",
      "• anatomy \n",
      "\n",
      "• high_school_psychology \n",
      "\n",
      "• high_school_geography \n",
      "\n",
      "• machine_learning \n",
      "\n",
      "• electrical_engineering \n",
      "\n",
      "• professional_law \n",
      "\n",
      "• security_studies \n",
      "\n",
      "• professional_psychology \n",
      "\n",
      "• college_chemistry \n",
      "\n",
      "• high_school_mathematics \n",
      "\n",
      "• miscellaneous \n",
      "\n",
      "• high_school_world_history \n",
      "\n",
      "• abstract_algebra \n",
      "\n",
      "• logical_fallacies \n",
      "\n",
      "• world_religions \n",
      "\n",
      "• high_school_government_and_politics \n",
      "\n",
      "• moral_disputes \n",
      "\n",
      "• high_school_computer_science \n",
      "\n",
      "• high_school_chemistry \n",
      "\n",
      "• sociology \n",
      "\n",
      "• high_school_european_history \n",
      "\n",
      "• conceptual_physics \n",
      "\n",
      "• high_school_macroeconomics \n",
      "\n",
      "• moral_scenarios \n",
      "\n",
      "• business_ethics \n",
      "\n",
      "• high_school_us_history \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Listing unique subjects in the dataset\n",
    "unique_subject = list_subjects(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3400631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subjects already evaluated: ['business_ethics', 'econometrics', 'high_school_computer_science', 'security_studies', 'electrical_engineering', 'philosophy', 'moral_scenarios', 'high_school_mathematics', 'prehistory', 'high_school_microeconomics', 'college_medicine', 'high_school_biology', 'abstract_algebra', 'medical_genetics', 'international_law', 'computer_security', 'formal_logic', 'college_computer_science', 'professional_psychology', 'high_school_chemistry', 'high_school_macroeconomics', 'us_foreign_policy', 'sociology', 'college_physics', 'jurisprudence', 'elementary_mathematics', 'nutrition', 'astronomy', 'marketing', 'high_school_us_history'] \n",
      "\n",
      "Topic: business_ethics\n",
      "\t • Number of examples: 5\n",
      "\t • Accuracy: 0.4\n",
      "Topic: econometrics\n",
      "\t • Number of examples: 9\n",
      "\t • Accuracy: 0.5555555555555556\n",
      "Topic: high_school_computer_science\n",
      "\t • Number of examples: 6\n",
      "\t • Accuracy: 1.0\n",
      "Topic: security_studies\n",
      "\t • Number of examples: 18\n",
      "\t • Accuracy: 0.5555555555555556\n",
      "Topic: electrical_engineering\n",
      "\t • Number of examples: 10\n",
      "\t • Accuracy: 0.4\n",
      "Topic: philosophy\n",
      "\t • Number of examples: 23\n",
      "\t • Accuracy: 0.782608695652174\n",
      "Topic: moral_scenarios\n",
      "\t • Number of examples: 68\n",
      "\t • Accuracy: 0.4411764705882353\n",
      "Topic: high_school_mathematics\n",
      "\t • Number of examples: 20\n",
      "\t • Accuracy: 0.7\n",
      "Topic: prehistory\n",
      "\t • Number of examples: 34\n",
      "\t • Accuracy: 0.7352941176470589\n",
      "Topic: high_school_microeconomics\n",
      "\t • Number of examples: 15\n",
      "\t • Accuracy: 0.8\n",
      "Topic: college_medicine\n",
      "\t • Number of examples: 12\n",
      "\t • Accuracy: 0.8333333333333334\n",
      "Topic: high_school_biology\n",
      "\t • Number of examples: 23\n",
      "\t • Accuracy: 0.782608695652174\n",
      "Topic: abstract_algebra\n",
      "\t • Number of examples: 11\n",
      "\t • Accuracy: 0.5454545454545454\n",
      "Topic: medical_genetics\n",
      "\t • Number of examples: 8\n",
      "\t • Accuracy: 0.75\n",
      "Topic: international_law\n",
      "\t • Number of examples: 9\n",
      "\t • Accuracy: 0.4444444444444444\n",
      "Topic: computer_security\n",
      "\t • Number of examples: 5\n",
      "\t • Accuracy: 0.6\n",
      "Topic: formal_logic\n",
      "\t • Number of examples: 10\n",
      "\t • Accuracy: 0.5\n",
      "Topic: college_computer_science\n",
      "\t • Number of examples: 8\n",
      "\t • Accuracy: 0.875\n",
      "Topic: professional_psychology\n",
      "\t • Number of examples: 37\n",
      "\t • Accuracy: 0.5945945945945946\n",
      "Topic: high_school_chemistry\n",
      "\t • Number of examples: 16\n",
      "\t • Accuracy: 0.625\n",
      "Topic: high_school_macroeconomics\n",
      "\t • Number of examples: 25\n",
      "\t • Accuracy: 0.56\n",
      "Topic: us_foreign_policy\n",
      "\t • Number of examples: 4\n",
      "\t • Accuracy: 0.5\n",
      "Topic: sociology\n",
      "\t • Number of examples: 11\n",
      "\t • Accuracy: 0.8181818181818182\n",
      "Topic: college_physics\n",
      "\t • Number of examples: 12\n",
      "\t • Accuracy: 0.75\n",
      "Topic: jurisprudence\n",
      "\t • Number of examples: 10\n",
      "\t • Accuracy: 0.6\n",
      "Topic: elementary_mathematics\n",
      "\t • Number of examples: 24\n",
      "\t • Accuracy: 1.0\n",
      "Topic: nutrition\n",
      "\t • Number of examples: 20\n",
      "\t • Accuracy: 0.8\n",
      "Topic: astronomy\n",
      "\t • Number of examples: 5\n",
      "\t • Accuracy: 0.6\n",
      "Topic: marketing\n",
      "\t • Number of examples: 17\n",
      "\t • Accuracy: 0.8235294117647058\n",
      "Topic: high_school_us_history\n",
      "\t • Number of examples: 16\n",
      "\t • Accuracy: 0.625\n"
     ]
    }
   ],
   "source": [
    "# Load the json file with results and displays its content\n",
    "result_path, results_dict = load_json_safely(config_dict)\n",
    "subjects_evaluated = display_results(results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bccc47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The random subjects of interest are: ['high_school_world_history', 'high_school_statistics', 'professional_law', 'virology', 'human_aging', 'machine_learning', 'high_school_european_history', 'management', 'high_school_physics', 'moral_disputes', 'professional_accounting', 'college_mathematics', 'anatomy', 'professional_medicine', 'miscellaneous', 'college_biology', 'human_sexuality', 'high_school_government_and_politics', 'logical_fallacies', 'global_facts', 'clinical_knowledge', 'conceptual_physics', 'public_relations', 'high_school_geography', 'college_chemistry', 'high_school_psychology', 'world_religions']\n"
     ]
    }
   ],
   "source": [
    "# Selecting n_sub random subjects from the remaining subjects\n",
    "subject_of_interest = random_subject(unique_subject, subjects_evaluated, config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35664cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random examples from the subjects of interest: ['high_school_world_history', 'high_school_statistics', 'professional_law', 'virology', 'human_aging', 'machine_learning', 'high_school_european_history', 'management', 'high_school_physics', 'moral_disputes', 'professional_accounting', 'college_mathematics', 'anatomy', 'professional_medicine', 'miscellaneous', 'college_biology', 'human_sexuality', 'high_school_government_and_politics', 'logical_fallacies', 'global_facts', 'clinical_knowledge', 'conceptual_physics', 'public_relations', 'high_school_geography', 'college_chemistry', 'high_school_psychology', 'world_religions'] \n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "The subject is high_school_statistics \n",
      "\n",
      "The question is: A major polling organization wants to predict the outcome of an upcoming national election (in terms of the proportion of voters who will vote for each candidate). They intend to use a 95% confidence interval with margin of error of no more than 2.5%. What is the minimum sample size needed to accomplish this goal? \n",
      "\n",
      "The options are:\n",
      "\n",
      "\t • 1536 \n",
      "\n",
      "\t • 39 \n",
      "\n",
      "\t • 1537 \n",
      "\n",
      "\t • 40 \n",
      "\n",
      "The answer is: 1537 \n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "The subject is conceptual_physics \n",
      "\n",
      "The question is: When the shadow of the Moon falls on Earth, we have a \n",
      "\n",
      "The options are:\n",
      "\n",
      "\t • lunar eclipse. \n",
      "\n",
      "\t • solar eclipse. \n",
      "\n",
      "\t • solar eclipse if it’s daytime and lunar eclipse if it’s nighttime. \n",
      "\n",
      "\t • very dangerous event. \n",
      "\n",
      "The answer is: solar eclipse. \n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check a few random examples for the subject of interest\n",
    "print_random_examples(dataset, subject_of_interest, n_ex=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cd305ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch status is False. Calling single_llm_eval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing high_school_world_history: 100%|██████████| 23/23 [12:12<00:00, 31.83s/it]\n",
      "Processing high_school_statistics:   6%|▋         | 1/16 [00:38<09:37, 38.49s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Run the LLM evaluation for the selected subject\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mllm_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubject_of_interest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Renaud\\Documents\\Renaud\\Travail\\Code\\llm_quantization_effect\\src\\llm_eval.py:98\u001b[0m, in \u001b[0;36mllm_eval\u001b[1;34m(dataset, result_path, model_dict, subject_of_interest, results_dict, config_dict)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch status is False. Calling single_llm_eval...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 98\u001b[0m     \u001b[43msingle_llm_eval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresult_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubject_of_interest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresults_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Renaud\\Documents\\Renaud\\Travail\\Code\\llm_quantization_effect\\src\\llm_eval.py:200\u001b[0m, in \u001b[0;36msingle_llm_eval\u001b[1;34m(dataset, result_path, model_dict, subject_of_interest, results_dict, config_dict)\u001b[0m\n\u001b[0;32m    197\u001b[0m subject_dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;28;01mlambda\u001b[39;00m x, s\u001b[38;5;241m=\u001b[39msubject: x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubject\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m s)\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# Iterate through a subject's dataset without batching\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43msingle_iteration\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf_log\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubject_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubject\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreferences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;66;03m# Calculate and store the accuracy for the subject\u001b[39;00m\n\u001b[0;32m    211\u001b[0m cal_and_store_subject_acc(\n\u001b[0;32m    212\u001b[0m     f_log,\n\u001b[0;32m    213\u001b[0m     result_path,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    220\u001b[0m     subject_dataset,\n\u001b[0;32m    221\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Renaud\\Documents\\Renaud\\Travail\\Code\\llm_quantization_effect\\src\\llm_eval.py:667\u001b[0m, in \u001b[0;36msingle_iteration\u001b[1;34m(f_log, model_dict, config_dict, subject_dataset, subject, references, predictions)\u001b[0m\n\u001b[0;32m    664\u001b[0m log_processing_item(f_log, subject, subject_dataset, i, log_prompt)\n\u001b[0;32m    666\u001b[0m \u001b[38;5;66;03m# Generate response for the single item\u001b[39;00m\n\u001b[1;32m--> 667\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43msingle_generate_llm_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;66;03m# Log response and extract prediction\u001b[39;00m\n\u001b[0;32m    670\u001b[0m log_response(f_log, response, correct_answer, references)\n",
      "File \u001b[1;32mc:\\Users\\Renaud\\Documents\\Renaud\\Travail\\Code\\llm_quantization_effect\\src\\llm_eval.py:810\u001b[0m, in \u001b[0;36msingle_generate_llm_response\u001b[1;34m(messages, model_dict, max_new_tokens)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;66;03m# Generate response for the single input\u001b[39;00m\n\u001b[0;32m    809\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# Ensure no gradients are calculated during inference\u001b[39;00m\n\u001b[1;32m--> 810\u001b[0m     generated_ids \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m    811\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs,\n\u001b[0;32m    812\u001b[0m         max_new_tokens\u001b[38;5;241m=\u001b[39mmax_new_tokens,\n\u001b[0;32m    813\u001b[0m     )\n\u001b[0;32m    815\u001b[0m \u001b[38;5;66;03m# Decode the generated IDs.\u001b[39;00m\n\u001b[0;32m    816\u001b[0m \u001b[38;5;66;03m# The generated_ids tensor will be of shape (1, sequence_length_including_input_and_generation)\u001b[39;00m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;66;03m# We slice from the original input_ids length to get only the generated part.\u001b[39;00m\n\u001b[0;32m    818\u001b[0m input_length \u001b[38;5;241m=\u001b[39m model_inputs\u001b[38;5;241m.\u001b[39minput_ids[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Renaud\\miniconda3\\envs\\corrupt_llm\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Renaud\\miniconda3\\envs\\corrupt_llm\\lib\\site-packages\\transformers\\generation\\utils.py:2597\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[0;32m   2589\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2590\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2591\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   2592\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2593\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2594\u001b[0m     )\n\u001b[0;32m   2596\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[1;32m-> 2597\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[0;32m   2598\u001b[0m         input_ids,\n\u001b[0;32m   2599\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2600\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2601\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2602\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   2603\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   2604\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2605\u001b[0m     )\n\u001b[0;32m   2607\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[0;32m   2608\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[0;32m   2609\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2610\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2611\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   2612\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2613\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2614\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Renaud\\miniconda3\\envs\\corrupt_llm\\lib\\site-packages\\transformers\\generation\\utils.py:3560\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   3558\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   3559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3560\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   3562\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[0;32m   3563\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[0;32m   3564\u001b[0m     outputs,\n\u001b[0;32m   3565\u001b[0m     model_kwargs,\n\u001b[0;32m   3566\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   3567\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Renaud\\miniconda3\\envs\\corrupt_llm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Renaud\\miniconda3\\envs\\corrupt_llm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Renaud\\miniconda3\\envs\\corrupt_llm\\lib\\site-packages\\transformers\\utils\\generic.py:969\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    966\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 969\u001b[0m     output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[0;32m    971\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[1;32mc:\\Users\\Renaud\\miniconda3\\envs\\corrupt_llm\\lib\\site-packages\\transformers\\models\\qwen3\\modeling_qwen3.py:730\u001b[0m, in \u001b[0;36mQwen3ForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    726\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[0;32m    727\u001b[0m )\n\u001b[0;32m    729\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m--> 730\u001b[0m outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[0;32m    731\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    732\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    733\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    734\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    735\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m    736\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    737\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    738\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    739\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    740\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    741\u001b[0m )\n\u001b[0;32m    743\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[0;32m    744\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Renaud\\miniconda3\\envs\\corrupt_llm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Renaud\\miniconda3\\envs\\corrupt_llm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Renaud\\miniconda3\\envs\\corrupt_llm\\lib\\site-packages\\transformers\\utils\\generic.py:969\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    966\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 969\u001b[0m     output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[0;32m    971\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[1;32mc:\\Users\\Renaud\\miniconda3\\envs\\corrupt_llm\\lib\\site-packages\\transformers\\models\\qwen3\\modeling_qwen3.py:463\u001b[0m, in \u001b[0;36mQwen3Model.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[0;32m    461\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[1;32m--> 463\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[0;32m    464\u001b[0m     hidden_states,\n\u001b[0;32m    465\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[0;32m    466\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    467\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    468\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    469\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    470\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    471\u001b[0m     position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[0;32m    472\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs,\n\u001b[0;32m    473\u001b[0m )\n\u001b[0;32m    475\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\Renaud\\miniconda3\\envs\\corrupt_llm\\lib\\site-packages\\transformers\\modeling_layers.py:48\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Renaud\\miniconda3\\envs\\corrupt_llm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Renaud\\miniconda3\\envs\\corrupt_llm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Renaud\\miniconda3\\envs\\corrupt_llm\\lib\\site-packages\\transformers\\models\\qwen3\\modeling_qwen3.py:284\u001b[0m, in \u001b[0;36mQwen3DecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    281\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[0;32m    283\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m--> 284\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[0;32m    285\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[0;32m    286\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    287\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    288\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[0;32m    289\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    290\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    291\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    292\u001b[0m     position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    294\u001b[0m )\n\u001b[0;32m    295\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    297\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Renaud\\miniconda3\\envs\\corrupt_llm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Renaud\\miniconda3\\envs\\corrupt_llm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Renaud\\miniconda3\\envs\\corrupt_llm\\lib\\site-packages\\transformers\\models\\qwen3\\modeling_qwen3.py:235\u001b[0m, in \u001b[0;36mQwen3Attention.forward\u001b[1;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m         attention_interface \u001b[38;5;241m=\u001b[39m ALL_ATTENTION_FUNCTIONS[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_attn_implementation]\n\u001b[1;32m--> 235\u001b[0m attn_output, attn_weights \u001b[38;5;241m=\u001b[39m attention_interface(\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    237\u001b[0m     query_states,\n\u001b[0;32m    238\u001b[0m     key_states,\n\u001b[0;32m    239\u001b[0m     value_states,\n\u001b[0;32m    240\u001b[0m     attention_mask,\n\u001b[0;32m    241\u001b[0m     dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_dropout,\n\u001b[0;32m    242\u001b[0m     scaling\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaling,\n\u001b[0;32m    243\u001b[0m     sliding_window\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msliding_window,  \u001b[38;5;66;03m# diff with Llama\u001b[39;00m\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    245\u001b[0m )\n\u001b[0;32m    247\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39minput_shape, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    248\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mo_proj(attn_output)\n",
      "File \u001b[1;32mc:\\Users\\Renaud\\miniconda3\\envs\\corrupt_llm\\lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:54\u001b[0m, in \u001b[0;36msdpa_attention_forward\u001b[1;34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(is_causal, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m     52\u001b[0m     is_causal \u001b[38;5;241m=\u001b[39m is_causal\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m---> 54\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run the LLM evaluation for the selected subject\n",
    "llm_eval(dataset, result_path, model_dict, subject_of_interest, results_dict, config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd19ece",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "corrupt_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
